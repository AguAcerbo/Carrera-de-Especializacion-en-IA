{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de PCA en NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "* Implementación de PCA en NumPy paso a paso\n",
    "* Comparación de resultados con Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dado un dataset $X \\in \\mathbb{R}^{n, d}$, con $n$ muestras y $d$ features, queremos reducir sus dimensiones a $m$. Para ello, el primer paso es centrar el dataset (Hint: usen np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cent_dataset(x):\n",
    "    \"\"\"\n",
    "    Esta funcion recibe un dataset y lo centra con el fin de que tenga media 0\n",
    "    Devuelve el data set centrado y la matriz con los valores de la media de cada uno\n",
    "    \"\"\"\n",
    "    media = x.mean(axis=1)\n",
    "    return x - media[:,None] , media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Obtener la matriz de covarianza de $X^T$, revisar en la teoría por qué utilizamos la transpuesta. Buscar en la documentación de NumPy qué funciones se pueden utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_dataset(x)\n",
    "    return x.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calcular los autovalores y autovectores de la matriz de covarianza. Revisar la documentación de NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covarianza_dataset(x)\n",
    "    s = x.dot(x.transpose()) / x.shape[0]\n",
    "    autovalores, autovectores = np.linalg.eigh(s)\n",
    "        # np.linalg.eigh():\n",
    "        # Return the eigenvalues and eigenvectors of a complex Hermitian\n",
    "            #(conjugate symmetric) or a real symmetric matrix.\n",
    "        # Returns two objects, a 1-D array containing the eigenvalues of a, \n",
    "            # and a 2-D square array or matrix (depending on the input type) \n",
    "            # of the corresponding eigenvectors (in columns).\n",
    "    return s, autovalores, autovectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ordernar los autovectores en el sentido de los autovalores decrecientes, revisar la teoría de ser necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_eig(autovalores, autovectores):\n",
    "    arg_sort = np.argsort(autovalores)[::-1]\n",
    "    autovalores = autovalores[arg_sort]\n",
    "    autovectores = autovectores[:,arg_sort]\n",
    "    return autovalores, autovectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Proyectar el dataset centrado sobre los $m$ autovectores más relevantes (Hint: usen np.dot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_B(m, autovectores):\n",
    "    \"\"\"m debe ser menor o igual al numero de columanas de la matriz de autovectores\"\"\"\n",
    "    return autovectores(:,:m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Consolidar los pasos anteriores en una función o clase PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_numpy(x):\n",
    "    # centro el dataset\n",
    "    media = x.mean(axis=1)\n",
    "    x_cent = x.copy()\n",
    "    x_cent = x_cent - media[:,None]\n",
    "    \n",
    "    s = x.dot(x.transpose()) / x.shape[0]\n",
    "    autovalores, autovectores = np.linalg.eigh(s)\n",
    "    \n",
    "    arg_sort = np.argsort(autovalores)[::-1]\n",
    "    autovalores = autovalores[arg_sort]\n",
    "    autovectores = autovectores[:,arg_sort]\n",
    "    \n",
    "    m=1\n",
    "    \n",
    "    B = autovectores[:,:m]\n",
    "    \n",
    "    z = np.dot(np.transpose(B),x)\n",
    "    x_recuperada = np.dot(B, z) + media\n",
    "\n",
    "    return B, z, x_recuperada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Comparar los resultados obtenidos con el modelo de PCA implementado en Scikit-learn ([ver documentación](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)). Tomar como dataset:\n",
    "\n",
    "$X=\\begin{bmatrix}\n",
    "0.8 & 0.7\\\\\n",
    "0.1 & -0.1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Se debe reducir a un componente. Verificar los resultados con np.testing.assert_allclose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.8,0.7],[0.1, -0.1]])\n",
    "\n",
    "B, z, x_recuperada = PCA_numpy(x)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
