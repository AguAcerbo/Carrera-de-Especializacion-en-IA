{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de053836",
   "metadata": {},
   "source": [
    "# CLASE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "804bfe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e33bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(muestras, features):\n",
    "    '''Crea un dataset'''\n",
    "    x = np.zeros((muestras,features))\n",
    "\n",
    "    for i in range(features):\n",
    "        mu = np.random.randint(100)\n",
    "        sigma = np.random.randint(1,10)\n",
    "        x[:,i] = np.random.normal(mu, sigma, muestras)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd6ea605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20.70209376 54.63194104 74.49308658 60.57973859 47.25614546]\n",
      " [24.88869157 56.35412167 87.2221691  62.71492393 46.79896584]\n",
      " [20.09744194 52.98132577 93.30437771 55.84635974 61.11785487]\n",
      " [17.50553403 51.18901362 94.01889585 59.63041367 48.24119493]\n",
      " [18.97642707 58.3524622  85.74181175 64.04590387 56.79016331]\n",
      " [12.59778916 56.16365846 93.6453944  53.40021319 36.88014476]\n",
      " [22.58263856 50.72326163 72.6818726  56.98618898 46.43480199]\n",
      " [20.70386241 45.39931589 92.71026339 61.53465474 45.62560986]\n",
      " [21.50218053 53.46240252 84.84869452 54.49655386 45.71092013]\n",
      " [24.84880025 50.60734015 81.73663875 58.34449354 51.47844892]]\n"
     ]
    }
   ],
   "source": [
    "x1 = dataset(10,5)\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97411291",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Normalización\n",
    "<p style='text-align: justify;'>\n",
    "    Muchos algoritmos de Machine Learning necesitan datos de entrada centrados y normalizados. Una normalización habitual es el z-score, que implica restarle la media y dividir por el desvío a cada feature de mi dataset.\n",
    "Dado un dataset X de n muestras y m features, implementar un método en numpy para normalizar con z-score. Pueden utilizar np.mean() y np.std().\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce11f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(x):\n",
    "    return (x - x.mean(axis=0))/x.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc5ca269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.63154495e-02  4.65887019e-01 -1.55425648e+00  5.39420840e-01\n",
      "  -2.18735689e-01]\n",
      " [ 1.29789733e+00  9.53497286e-01  1.59076703e-01  1.17163467e+00\n",
      "  -2.91343679e-01]\n",
      " [-1.00112223e-01 -1.46056181e-03  9.77741350e-01 -8.62100379e-01\n",
      "   1.98274227e+00]\n",
      " [-8.56389252e-01 -5.08927540e-01  1.07391542e+00  2.58332201e-01\n",
      "  -6.22928848e-02]\n",
      " [-4.27206328e-01  1.51929819e+00 -4.01792414e-02  1.56572876e+00\n",
      "   1.29543039e+00]\n",
      " [-2.28839025e+00  8.99570411e-01  1.02364216e+00 -1.58638770e+00\n",
      "  -1.86662313e+00]\n",
      " [ 6.25028168e-01 -6.40798408e-01 -1.79804569e+00 -5.24604709e-01\n",
      "  -3.49179165e-01]\n",
      " [ 7.68315120e-02 -2.14819580e+00  8.97773624e-01  8.22165003e-01\n",
      "  -4.77692798e-01]\n",
      " [ 3.09767898e-01  1.34749287e-01 -1.60392728e-01 -1.26176870e+00\n",
      "  -4.64144059e-01]\n",
      " [ 1.28625769e+00 -6.73619880e-01 -5.79275111e-01 -1.22419984e-01\n",
      "   4.51838749e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(zscore(x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2ae59",
   "metadata": {},
   "source": [
    "### Agregar elementos NaN aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff1fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nanvalues(data):\n",
    "    \n",
    "    \"\"\"Esta funcion agrega valores NaN a un dataset de forma aleatoria\"\"\"\n",
    "    \n",
    "    muestras, features = data.shape\n",
    "    \n",
    "    for feature in range(features):\n",
    "        #Cantidad de valores NaN que tendra el feature\n",
    "        value = np.random.randint(1,10)\n",
    "        \n",
    "        idx = np.random.randint(0,muestras,(muestras//value,1))\n",
    "        data[np.unique(idx),feature] = np.NaN\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be07d10",
   "metadata": {},
   "source": [
    "## NaN evaluation\n",
    "\n",
    "### Ejercicio 2: Remover filas y columnas con NaNs en un dataset\n",
    "<p style='text-align: justify;'>\n",
    "    Dado un dataset, hacer una función que, utilizando numpy, filtre las columnas y las filas que tienen NaNs.\n",
    "</p>\n",
    "\n",
    "### Ejercicio 3: Reemplazar NaNs por la media de la columna.\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "    Dado un dataset, hacer una función que utilizando numpy reemplace los NaNs por la media de la columna.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed7555ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_evaluation(dataset, threshold_muestra = 0.5, threshold_feature = 0.8):\n",
    "    \"\"\"\n",
    "    1ro: Elimina las muestras del dataset que superan threshold_muestra*nro_feature en cantidad de NaN.\n",
    "    \n",
    "    2do: Elimina los features del dataset que superen threshold_feature*muestras en cantidad de NaN.\n",
    "     \n",
    "    3ro: Si la cantidad de NaN es menor entonces lo reemplza por la media.\n",
    "    \"\"\"\n",
    "    dataset_bool = np.isnan(dataset)\n",
    "    clean_dataset = np.delete(dataset, np.argwhere((np.sum(dataset_bool, axis=1) >= threshold_muestra*dataset.shape[1]) == True), axis = 0)\n",
    "    \n",
    "    dataset_bool = np.isnan(clean_dataset)\n",
    "    clean_dataset = np.delete(clean_dataset, np.argwhere((np.sum(dataset_bool, axis=0) >= threshold_feature*clean_dataset.shape[0]) == True), axis = 1)\n",
    "\n",
    "    dataset_bool = np.isnan(clean_dataset)\n",
    "    for feature in range(clean_dataset.shape[1]):\n",
    "        clean_dataset[dataset_bool[:,feature]==True, feature] = clean_dataset[dataset_bool[:,feature]==False, feature].mean()\n",
    "    \n",
    "    return clean_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "539a55db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[81.02958774 48.21816557         nan 43.0005395          nan]\n",
      " [78.93434425 58.03148234         nan 62.22884721 79.14438956]\n",
      " [85.53786527 61.5346191   2.85029102         nan 75.40945975]\n",
      " [82.32788487 58.68490036  5.8875918  58.22953286 87.00095331]\n",
      " [81.97409904         nan 14.13949026 63.27787881 73.5241419 ]\n",
      " [        nan 69.79724053  7.15108613 60.61837824 80.47905383]\n",
      " [82.23760661         nan -3.9843835  62.22631185 80.63096396]\n",
      " [        nan 53.31525125 -9.03380871 63.83950278 79.6846061 ]\n",
      " [80.55041095 51.54523339  4.3196903  48.9569486  69.24227006]\n",
      " [80.05982741 44.35147439  5.03676045 50.04954785 79.22540958]\n",
      " [83.57496714 46.0094247   7.33383024 59.98846646         nan]\n",
      " [83.38247            nan 17.48444688         nan 75.01128405]\n",
      " [83.34025533         nan 11.69919448 66.34343549 72.37160133]\n",
      " [81.4315373          nan  3.84080597 64.86721018         nan]\n",
      " [82.52821912         nan  0.77216856 61.65060023 59.47692799]\n",
      " [88.82924155         nan  5.6798407  62.95072562 77.48431676]\n",
      " [84.61489386         nan  2.86942177 45.36610124 75.34118046]\n",
      " [82.68782198 58.86359539  5.07572779 60.84886314 81.15759859]\n",
      " [78.78615931 53.17789098         nan 60.85412691         nan]\n",
      " [79.28216676 53.82992918         nan 55.67993442         nan]]\n"
     ]
    }
   ],
   "source": [
    "x = nanvalues(dataset(20,5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f16fb77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 5)\n",
      "\n",
      "[[81.02958774 48.21816557  5.07013463 43.0005395  76.34561048]\n",
      " [78.93434425 58.03148234  5.07013463 62.22884721 79.14438956]\n",
      " [85.53786527 61.5346191   2.85029102 58.38760841 75.40945975]\n",
      " [82.32788487 58.68490036  5.8875918  58.22953286 87.00095331]\n",
      " [81.97409904 54.77993393 14.13949026 63.27787881 73.5241419 ]\n",
      " [82.28385325 69.79724053  7.15108613 60.61837824 80.47905383]\n",
      " [82.23760661 54.77993393 -3.9843835  62.22631185 80.63096396]\n",
      " [82.28385325 53.31525125 -9.03380871 63.83950278 79.6846061 ]\n",
      " [80.55041095 51.54523339  4.3196903  48.9569486  69.24227006]\n",
      " [80.05982741 44.35147439  5.03676045 50.04954785 79.22540958]\n",
      " [83.57496714 46.0094247   7.33383024 59.98846646 76.34561048]\n",
      " [83.38247    54.77993393 17.48444688 58.38760841 75.01128405]\n",
      " [83.34025533 54.77993393 11.69919448 66.34343549 72.37160133]\n",
      " [81.4315373  54.77993393  3.84080597 64.86721018 76.34561048]\n",
      " [82.52821912 54.77993393  0.77216856 61.65060023 59.47692799]\n",
      " [88.82924155 54.77993393  5.6798407  62.95072562 77.48431676]\n",
      " [84.61489386 54.77993393  2.86942177 45.36610124 75.34118046]\n",
      " [82.68782198 58.86359539  5.07572779 60.84886314 81.15759859]\n",
      " [78.78615931 53.17789098  5.07013463 60.85412691 76.34561048]\n",
      " [79.28216676 53.82992918  5.07013463 55.67993442 76.34561048]]\n"
     ]
    }
   ],
   "source": [
    "clean_dataset = nan_evaluation(x, 0.5, 0.8)\n",
    "print(clean_dataset.shape)\n",
    "print()\n",
    "print(clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7090e0fe",
   "metadata": {},
   "source": [
    "### Ejercicio 4: Dado un dataset X separarlo en 70 / 20 / 10\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "    Como vimos en el ejercicio integrador, en problemas de Machine Learning es fundamental que separemos los datasets de n muestras, en 3 datasets de la siguiente manera:\n",
    "    \n",
    "* Training dataset: los datos que utilizaremos para entrenar nuestros modelos. Ej: 70% de las muestras.\n",
    "* Validation dataset: los datos que usamos para calcular métricas y ajustar los hiperparámetros de nuestros modelos. Ej: 20% de las muestras.\n",
    "* Testing dataset: una vez que entrenamos los modelos y encontramos los hiperparámetros óptimos de los mísmos, el testing dataset se lo utiliza para computar las métricas finales de nuestros modelos y analizar cómo se comporta respecto a la generalización. Ej: 10% de las muestras.\n",
    "    \n",
    "A partir de utilizar np.random.permutation, hacer un método que dado un dataset, devuelva los 3 datasets\n",
    "como nuevos numpy arrays.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e96ba035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test(dataset):\n",
    "    \"\"\"Separa el dataset en training dataset, validation dataset y test dataset con proporciones 70%/20%/10% respectivament \"\"\"\n",
    "    dataset_index = np.random.permutation(dataset.shape[0])\n",
    "    \n",
    "    train_data_idx = np.uint(np.around(dataset.shape[0]*70/100))\n",
    "    val_data_idx = np.uint(np.around(dataset.shape[0]*20/100))\n",
    "    test_data_idx = np.uint(dataset.shape[0]) - train_data_idx - val_data_idx\n",
    "\n",
    "    train_data = dataset[dataset_index[0:train_data_idx]]\n",
    "    val_data = dataset[dataset_index[train_data_idx:train_data_idx+val_data_idx]]\n",
    "    test_data = dataset[dataset_index[train_data_idx+val_data_idx:train_data_idx+val_data_idx+test_data_idx]]\n",
    "    \n",
    "    return train_data,val_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c79f2db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset de entrenamiento posee dimensiones (14, 5):\n",
      " [[84.61489386 54.77993393  2.86942177 45.36610124 75.34118046]\n",
      " [82.52821912 54.77993393  0.77216856 61.65060023 59.47692799]\n",
      " [78.78615931 53.17789098  5.07013463 60.85412691 76.34561048]\n",
      " [82.32788487 58.68490036  5.8875918  58.22953286 87.00095331]\n",
      " [82.23760661 54.77993393 -3.9843835  62.22631185 80.63096396]\n",
      " [79.28216676 53.82992918  5.07013463 55.67993442 76.34561048]\n",
      " [81.4315373  54.77993393  3.84080597 64.86721018 76.34561048]\n",
      " [82.68782198 58.86359539  5.07572779 60.84886314 81.15759859]\n",
      " [82.28385325 53.31525125 -9.03380871 63.83950278 79.6846061 ]\n",
      " [83.38247    54.77993393 17.48444688 58.38760841 75.01128405]\n",
      " [88.82924155 54.77993393  5.6798407  62.95072562 77.48431676]\n",
      " [83.57496714 46.0094247   7.33383024 59.98846646 76.34561048]\n",
      " [82.28385325 69.79724053  7.15108613 60.61837824 80.47905383]\n",
      " [85.53786527 61.5346191   2.85029102 58.38760841 75.40945975]]\n",
      "\n",
      "Dataset de validacion posee dimensiones (4, 5):\n",
      " [[81.97409904 54.77993393 14.13949026 63.27787881 73.5241419 ]\n",
      " [81.02958774 48.21816557  5.07013463 43.0005395  76.34561048]\n",
      " [80.05982741 44.35147439  5.03676045 50.04954785 79.22540958]\n",
      " [78.93434425 58.03148234  5.07013463 62.22884721 79.14438956]]\n",
      "\n",
      "Dataset de testeo posee dimensiones (2, 5):\n",
      " [[83.34025533 54.77993393 11.69919448 66.34343549 72.37160133]\n",
      " [80.55041095 51.54523339  4.3196903  48.9569486  69.24227006]]\n"
     ]
    }
   ],
   "source": [
    "train_dataset,val_dataset,test_dataset = train_val_test(clean_dataset)\n",
    "\n",
    "print(f\"\\nDataset de entrenamiento posee dimensiones {train_dataset.shape}:\\n {train_dataset}\")\n",
    "print(f\"\\nDataset de validacion posee dimensiones {val_dataset.shape}:\\n {val_dataset}\")\n",
    "print(f\"\\nDataset de testeo posee dimensiones {test_dataset.shape}:\\n {test_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db376e53",
   "metadata": {},
   "source": [
    "#### Ejercicio 5:   A partir del dataset de consigna, aplicar los conceptos de regresión lineal.\n",
    "1. Armar una clase para cargar el [dataset](data/income.csv) en un ndarray estructurado, tal como se realizó en el ejercicio 10 de la Clase 1.\n",
    "2. Incluir un método split a la clase para obtener los sets de training y test.\n",
    "3. Crear una clase métrica base y una clase MSE (Error cuadrático medio) que herede de la clase base.\n",
    "4. Crear una clase modelo base y clases regresión lineal y regresión afín que hereden de la primera. Usar los conocimientos teóricos vistos en clase.\n",
    "5. Hacer un fit de las regresiones con los datos de entrenamiento.\n",
    "6. Hacer un predict sobre los datos de test y reportar el MSE en cada caso.\n",
    "7. Graficar la curva obtenida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d6ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
