{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de053836",
   "metadata": {},
   "source": [
    "# CLASE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "804bfe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e33bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(muestras, features):\n",
    "    '''Crea un dataset'''\n",
    "    x = np.zeros((muestras,features))\n",
    "\n",
    "    for i in range(features):\n",
    "        mu = np.random.randint(100)\n",
    "        sigma = np.random.randint(1,10)\n",
    "        x[:,i] = np.random.normal(mu, sigma, muestras)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd6ea605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 88.5919048   13.5413879   18.83660565  11.09455474  42.98690405]\n",
      " [ 92.98946762  13.57741122  30.20894611   7.91266321  32.98289363]\n",
      " [ 91.68956865   9.31771835  22.28159535   4.71989714  52.07429774]\n",
      " [ 97.99200936   9.57837575  33.74408346   6.91819735  47.22575141]\n",
      " [ 94.02222379  12.32465952  24.19991352   7.26835082  41.01093492]\n",
      " [ 93.44974974  14.63832719  31.62269138   8.63243247  44.20264211]\n",
      " [108.56991748   7.96087085  27.05407583   8.74809618  37.41597726]\n",
      " [ 84.64110919   4.31206634  26.07511019   9.61613165  38.90229471]\n",
      " [101.74147302   8.56601659  15.18437887   9.25110121  38.32325977]\n",
      " [104.38343349  10.43789705  32.57778495   8.99047368  33.03647988]]\n"
     ]
    }
   ],
   "source": [
    "x1 = dataset(10,5)\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97411291",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Normalización\n",
    "<p style='text-align: justify;'>\n",
    "    Muchos algoritmos de Machine Learning necesitan datos de entrada centrados y normalizados. Una normalización habitual es el z-score, que implica restarle la media y dividir por el desvío a cada feature de mi dataset.\n",
    "Dado un dataset X de n muestras y m features, implementar un método en numpy para normalizar con z-score. Pueden utilizar np.mean() y np.std().\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce11f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(x):\n",
    "    return (x - x.mean(axis=0))/x.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc5ca269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.03411388  1.03950997 -1.26149571  1.68607687  0.37939633]\n",
      " [-0.40383436  1.05152782  0.69251259 -0.24418919 -1.36906226]\n",
      " [-0.59014203 -0.36956148 -0.66957372 -2.18105219  1.96765252]\n",
      " [ 0.3131536  -0.28260276  1.29992386 -0.84747301  1.12024412]\n",
      " [-0.25581486  0.63359346 -0.33996613 -0.63505554  0.03404481]\n",
      " [-0.33786455  1.40546328  0.93542389  0.1924524   0.59187788]\n",
      " [ 1.82922946 -0.82222356  0.15043924  0.26261876 -0.59426667]\n",
      " [-1.60036061 -2.03951251 -0.01776774  0.78920472 -0.3344944 ]\n",
      " [ 0.85054448 -0.62033902 -1.88902542  0.56776226 -0.43569567]\n",
      " [ 1.22920276  0.0041448   1.09952914  0.4096549  -1.35969668]]\n"
     ]
    }
   ],
   "source": [
    "print(zscore(x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2ae59",
   "metadata": {},
   "source": [
    "### Agregar elementos NaN aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff1fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nanvalues(data):\n",
    "    \n",
    "    \"\"\"Esta funcion agrega valores NaN a un dataset de forma aleatoria\"\"\"\n",
    "    \n",
    "    muestras, features = data.shape\n",
    "    \n",
    "    for feature in range(features):\n",
    "        #Cantidad de valores NaN que tendra el feature\n",
    "        value = np.random.randint(1,10)\n",
    "        \n",
    "        idx = np.random.randint(0,muestras,(muestras//value,1))\n",
    "        data[np.unique(idx),feature] = np.NaN\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be07d10",
   "metadata": {},
   "source": [
    "## NaN evaluation\n",
    "\n",
    "### Ejercicio 2: Remover filas y columnas con NaNs en un dataset\n",
    "<p style='text-align: justify;'>\n",
    "    Dado un dataset, hacer una función que, utilizando numpy, filtre las columnas y las filas que tienen NaNs.\n",
    "</p>\n",
    "\n",
    "### Ejercicio 3: Reemplazar NaNs por la media de la columna.\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "    Dado un dataset, hacer una función que utilizando numpy reemplace los NaNs por la media de la columna.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed7555ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_evaluation(dataset, threshold_muestra = 0.5, threshold_feature = 0.8):\n",
    "    \"\"\"\n",
    "    1ro: Elimina las muestras del dataset que superan threshold_muestra*nro_feature en cantidad de NaN.\n",
    "    \n",
    "    2do: Elimina los features del dataset que superen threshold_feature*muestras en cantidad de NaN.\n",
    "     \n",
    "    3ro: Si la cantidad de NaN es menor entonces lo reemplza por la media.\n",
    "    \"\"\"\n",
    "    dataset_bool = np.isnan(dataset)\n",
    "    clean_dataset = np.delete(dataset, np.argwhere((np.sum(dataset_bool, axis=1) >= threshold_muestra*dataset.shape[1]) == True), axis = 0)\n",
    "    \n",
    "    dataset_bool = np.isnan(clean_dataset)\n",
    "    clean_dataset = np.delete(clean_dataset, np.argwhere((np.sum(dataset_bool, axis=0) >= threshold_feature*clean_dataset.shape[0]) == True), axis = 1)\n",
    "\n",
    "    dataset_bool = np.isnan(clean_dataset)\n",
    "    for feature in range(clean_dataset.shape[1]):\n",
    "        clean_dataset[dataset_bool[:,feature]==True, feature] = clean_dataset[dataset_bool[:,feature]==False, feature].mean()\n",
    "    \n",
    "    return clean_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "539a55db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.83499991e+01 -4.51558499e+00  1.05306650e+02  9.68364170e+01\n",
      "   1.10795800e+02]\n",
      " [ 1.00487645e+02  4.81791098e+00             nan  9.58462949e+01\n",
      "   9.55186949e+01]\n",
      " [            nan -7.45764946e+00  1.03284915e+02  9.72033423e+01\n",
      "   9.75963982e+01]\n",
      " [ 8.75021705e+01 -8.22185585e+00  1.00292557e+02  1.00230083e+02\n",
      "   9.21561178e+01]\n",
      " [ 9.96255190e+01             nan  9.16885840e+01  1.00014112e+02\n",
      "              nan]\n",
      " [ 1.06377191e+02 -1.38684642e+00  8.56271982e+01  1.00506983e+02\n",
      "   1.02877899e+02]\n",
      " [ 1.07510958e+02 -1.56514358e+00  9.40090433e+01  9.97190559e+01\n",
      "              nan]\n",
      " [ 1.00262576e+02 -4.29647542e+00  9.43184173e+01  1.03584677e+02\n",
      "   1.00274414e+02]\n",
      " [ 8.98534317e+01  5.35177635e+00  8.79551308e+01  1.00481694e+02\n",
      "   1.06445656e+02]\n",
      " [ 1.10551003e+02  1.58748539e+01  9.50789755e+01  1.01506144e+02\n",
      "   9.31519712e+01]\n",
      " [ 1.10889773e+02  7.89210463e-02  9.17980574e+01  1.00918665e+02\n",
      "   1.07675552e+02]\n",
      " [            nan -3.01138597e+00             nan  9.95352498e+01\n",
      "   1.03623260e+02]\n",
      " [ 9.72565340e+01 -1.07261707e+01  9.12490439e+01  9.90363843e+01\n",
      "   1.03763118e+02]\n",
      " [ 9.40824725e+01  4.50895415e+00  9.93147360e+01  9.18892337e+01\n",
      "   1.02205052e+02]\n",
      " [ 9.75889892e+01 -6.64506691e+00  8.84492441e+01  9.94974840e+01\n",
      "   1.14790633e+02]\n",
      " [ 1.06243695e+02  8.80501496e-01  9.42091052e+01  9.69718463e+01\n",
      "   9.49333673e+01]\n",
      " [ 9.99362831e+01             nan  9.81331440e+01  9.89599428e+01\n",
      "   1.01650932e+02]\n",
      " [ 1.03811615e+02 -4.26081051e+00  1.02277386e+02  1.00589075e+02\n",
      "   8.82372431e+01]\n",
      " [ 9.28242155e+01  1.28181734e+01  9.56974349e+01             nan\n",
      "   9.62402605e+01]\n",
      " [ 9.84919452e+01 -1.40143758e+01  1.01092277e+02  1.00248698e+02\n",
      "   8.86999523e+01]]\n"
     ]
    }
   ],
   "source": [
    "x = nanvalues(dataset(20,5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f16fb77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 5)\n",
      "\n",
      "[[ 9.83499991e+01 -4.51558499e+00  1.05306650e+02  9.68364170e+01\n",
      "   1.10795800e+02]\n",
      " [ 1.00487645e+02  4.81791098e+00  9.55434389e+01  9.58462949e+01\n",
      "   9.55186949e+01]\n",
      " [ 1.00091445e+02 -7.45764946e+00  1.03284915e+02  9.72033423e+01\n",
      "   9.75963982e+01]\n",
      " [ 8.75021705e+01 -8.22185585e+00  1.00292557e+02  1.00230083e+02\n",
      "   9.21561178e+01]\n",
      " [ 9.96255190e+01 -1.20945968e+00  9.16885840e+01  1.00014112e+02\n",
      "   1.00035351e+02]\n",
      " [ 1.06377191e+02 -1.38684642e+00  8.56271982e+01  1.00506983e+02\n",
      "   1.02877899e+02]\n",
      " [ 1.07510958e+02 -1.56514358e+00  9.40090433e+01  9.97190559e+01\n",
      "   1.00035351e+02]\n",
      " [ 1.00262576e+02 -4.29647542e+00  9.43184173e+01  1.03584677e+02\n",
      "   1.00274414e+02]\n",
      " [ 8.98534317e+01  5.35177635e+00  8.79551308e+01  1.00481694e+02\n",
      "   1.06445656e+02]\n",
      " [ 1.10551003e+02  1.58748539e+01  9.50789755e+01  1.01506144e+02\n",
      "   9.31519712e+01]\n",
      " [ 1.10889773e+02  7.89210463e-02  9.17980574e+01  1.00918665e+02\n",
      "   1.07675552e+02]\n",
      " [ 1.00091445e+02 -3.01138597e+00  9.55434389e+01  9.95352498e+01\n",
      "   1.03623260e+02]\n",
      " [ 9.72565340e+01 -1.07261707e+01  9.12490439e+01  9.90363843e+01\n",
      "   1.03763118e+02]\n",
      " [ 9.40824725e+01  4.50895415e+00  9.93147360e+01  9.18892337e+01\n",
      "   1.02205052e+02]\n",
      " [ 9.75889892e+01 -6.64506691e+00  8.84492441e+01  9.94974840e+01\n",
      "   1.14790633e+02]\n",
      " [ 1.06243695e+02  8.80501496e-01  9.42091052e+01  9.69718463e+01\n",
      "   9.49333673e+01]\n",
      " [ 9.99362831e+01 -1.20945968e+00  9.81331440e+01  9.89599428e+01\n",
      "   1.01650932e+02]\n",
      " [ 1.03811615e+02 -4.26081051e+00  1.02277386e+02  1.00589075e+02\n",
      "   8.82372431e+01]\n",
      " [ 9.28242155e+01  1.28181734e+01  9.56974349e+01  9.91355464e+01\n",
      "   9.62402605e+01]\n",
      " [ 9.84919452e+01 -1.40143758e+01  1.01092277e+02  1.00248698e+02\n",
      "   8.86999523e+01]]\n"
     ]
    }
   ],
   "source": [
    "clean_dataset = nan_evaluation(x, 0.5, 0.8)\n",
    "print(clean_dataset.shape)\n",
    "print()\n",
    "print(clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7090e0fe",
   "metadata": {},
   "source": [
    "### Ejercicio 4: Dado un dataset X separarlo en 70 / 20 / 10\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "    Como vimos en el ejercicio integrador, en problemas de Machine Learning es fundamental que separemos los datasets de n muestras, en 3 datasets de la siguiente manera:\n",
    "    \n",
    "* Training dataset: los datos que utilizaremos para entrenar nuestros modelos. Ej: 70% de las muestras.\n",
    "* Validation dataset: los datos que usamos para calcular métricas y ajustar los hiperparámetros de nuestros modelos. Ej: 20% de las muestras.\n",
    "* Testing dataset: una vez que entrenamos los modelos y encontramos los hiperparámetros óptimos de los mísmos, el testing dataset se lo utiliza para computar las métricas finales de nuestros modelos y analizar cómo se comporta respecto a la generalización. Ej: 10% de las muestras.\n",
    "    \n",
    "A partir de utilizar np.random.permutation, hacer un método que dado un dataset, devuelva los 3 datasets\n",
    "como nuevos numpy arrays.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e96ba035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test(dataset):\n",
    "    \"\"\"Separa el dataset en training dataset, validation dataset y test dataset con proporciones 70%/20%/10% respectivament \"\"\"\n",
    "    dataset_index = np.random.permutation(dataset.shape[0])\n",
    "    \n",
    "    train_data_idx = np.uint(np.around(dataset.shape[0]*70/100))\n",
    "    val_data_idx = np.uint(np.around(dataset.shape[0]*20/100))\n",
    "    test_data_idx = np.uint(dataset.sha pe[0]) - train_data_idx - val_data_idx\n",
    "\n",
    "    train_data = clean_dataset[dataset_index[0:train_data_idx]]\n",
    "    val_data = clean_dataset[dataset_index[train_data_idx:train_data_idx+val_data_idx]]\n",
    "    test_data = clean_dataset[dataset_index[train_data_idx+val_data_idx:train_data_idx+val_data_idx+test_data_idx]]\n",
    "    \n",
    "    return train_data,val_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c79f2db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset de entrenamiento posee dimensiones (14, 5):\n",
      " [[ 1.00262576e+02 -4.29647542e+00  9.43184173e+01  1.03584677e+02\n",
      "   1.00274414e+02]\n",
      " [ 1.00091445e+02 -3.01138597e+00  9.55434389e+01  9.95352498e+01\n",
      "   1.03623260e+02]\n",
      " [ 9.96255190e+01 -1.20945968e+00  9.16885840e+01  1.00014112e+02\n",
      "   1.00035351e+02]\n",
      " [ 9.72565340e+01 -1.07261707e+01  9.12490439e+01  9.90363843e+01\n",
      "   1.03763118e+02]\n",
      " [ 1.10889773e+02  7.89210463e-02  9.17980574e+01  1.00918665e+02\n",
      "   1.07675552e+02]\n",
      " [ 9.84919452e+01 -1.40143758e+01  1.01092277e+02  1.00248698e+02\n",
      "   8.86999523e+01]\n",
      " [ 1.06243695e+02  8.80501496e-01  9.42091052e+01  9.69718463e+01\n",
      "   9.49333673e+01]\n",
      " [ 1.00487645e+02  4.81791098e+00  9.55434389e+01  9.58462949e+01\n",
      "   9.55186949e+01]\n",
      " [ 9.75889892e+01 -6.64506691e+00  8.84492441e+01  9.94974840e+01\n",
      "   1.14790633e+02]\n",
      " [ 8.98534317e+01  5.35177635e+00  8.79551308e+01  1.00481694e+02\n",
      "   1.06445656e+02]\n",
      " [ 9.83499991e+01 -4.51558499e+00  1.05306650e+02  9.68364170e+01\n",
      "   1.10795800e+02]\n",
      " [ 1.00091445e+02 -7.45764946e+00  1.03284915e+02  9.72033423e+01\n",
      "   9.75963982e+01]\n",
      " [ 9.28242155e+01  1.28181734e+01  9.56974349e+01  9.91355464e+01\n",
      "   9.62402605e+01]\n",
      " [ 9.40824725e+01  4.50895415e+00  9.93147360e+01  9.18892337e+01\n",
      "   1.02205052e+02]]\n",
      "\n",
      "Dataset de validacion posee dimensiones (4, 5):\n",
      " [[ 87.50217049  -8.22185585 100.29255747 100.23008267  92.15611777]\n",
      " [110.55100257  15.87485389  95.07897549 101.50614388  93.15197124]\n",
      " [ 99.93628307  -1.20945968  98.13314404  98.9599428  101.65093248]\n",
      " [107.5109583   -1.56514358  94.0090433   99.71905592 100.03535111]]\n",
      "\n",
      "Dataset de testeo posee dimensiones (2, 5):\n",
      " [[103.81161514  -4.26081051 102.27738581 100.58907521  88.23724311]\n",
      " [106.37719082  -1.38684642  85.62719816 100.50698261 102.87789904]]\n"
     ]
    }
   ],
   "source": [
    "train_dataset,val_dataset,test_dataset = train_val_test(clean_dataset)\n",
    "\n",
    "print(f\"\\nDataset de entrenamiento posee dimensiones {train_dataset.shape}:\\n {train_dataset}\")\n",
    "print(f\"\\nDataset de validacion posee dimensiones {val_dataset.shape}:\\n {val_dataset}\")\n",
    "print(f\"\\nDataset de testeo posee dimensiones {test_dataset.shape}:\\n {test_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db376e53",
   "metadata": {},
   "source": [
    "#### Ejercicio 5:   A partir del dataset de consigna, aplicar los conceptos de regresión lineal.\n",
    "1. Armar una clase para cargar el [dataset](data/income.csv) en un ndarray estructurado, tal como se realizó en el ejercicio 10 de la Clase 1.\n",
    "2. Incluir un método split a la clase para obtener los sets de training y test.\n",
    "3. Crear una clase métrica base y una clase MSE (Error cuadrático medio) que herede de la clase base.\n",
    "4. Crear una clase modelo base y clases regresión lineal y regresión afín que hereden de la primera. Usar los conocimientos teóricos vistos en clase.\n",
    "5. Hacer un fit de las regresiones con los datos de entrenamiento.\n",
    "6. Hacer un predict sobre los datos de test y reportar el MSE en cada caso.\n",
    "7. Graficar la curva obtenida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d6ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
