{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de éste ejercicio es que implementen paso a paso los building blocks del modelo de regresión logística, para finalmente crear una clase del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos las Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de Building Blocks del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se deberán implementar paso a paso los distintos bloques de código que conforman el modelo, junto con algunas funciones auxiliares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar la función: $g(z) = \\frac{1}{1 + e^{-z}}$ en NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_fuction(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar la función de costo: $J(w) = \\frac{1}{n}\\sum_{i=1}^{n}L\\left ( \\hat{y},y \\right )= \\frac{-1}{n}\\sum_{i=1}^{n}\\left [y^{(i)}log(\\hat{y}^{(i)})+ (1-y^{(i)})log(1-\\hat{y}^{(i)}) \\right ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costf_lr(y, y_hat):\n",
    "    return (-1)*np.mean(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar el gradiente de la función costo respecto de los parámetros: $\\frac{\\partial J(w)}{\\partial w} = \\frac{2}{n}\\sum_{i=1}^{n}\\left ( \\hat{y}^{i}-y^{i}\\right )\\bar{x}^i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_lr(y, y_hat, x):\n",
    "    # y* = (y_hat-y) dimensiones nx1\n",
    "    # x dimensiones nxm\n",
    "    # np.dot(y*.T,x) = y*1 . x1 + ... + y*n . xn \n",
    "    return np.mean((y_hat-y).T.dot(x)) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar normalización Z-score de las features de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    # centro el dataset\n",
    "    x_cent = x.copy()\n",
    "    return (x_cent - np.mean(x, axis=0))/np.std(x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas (Precision, Recall y Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar las métricas en NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_pra(truth, pred):\n",
    "    # Calculo el valor True Positive\n",
    "    true_positive = sum(np.logical_and(truth, pred))\n",
    "\n",
    "    # Calculo el valor True Negative\n",
    "    true_negative = sum(np.logical_and(np.logical_not(truth), np.logical_not(pred)))\n",
    "\n",
    "    # Calculo el valor False Negative\n",
    "    false_negative = sum(np.logical_and(truth, np.logical_not(pred)))\n",
    "\n",
    "    # Calculo el valor False Positive\n",
    "    false_positive = sum(np.logical_and(np.logical_not(truth), pred))\n",
    "\n",
    "    # Metricas\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n",
    "\n",
    "    return true_positive, true_negative, false_negative, false_positive, precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementar función fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizas los bloques anteriores, junto con la implementación en NumPy del algoritmo Mini-Batch gradient descent, para crear la función fit de nuestro modelo de regresión logística. Cada un determinado número de epochs calculen el loss, almacénenlo en una lista y hagan un log de los valores. La función debe devolver los parámetros ajustados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def fit(self, x, y, lr, b, epochs, bias=True):\n",
    "    \n",
    "    cost = []\n",
    "    \n",
    "    if bias:\n",
    "        x = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "    \n",
    "    w = np.random.randn(m).reshape(x.shape[1], 1)\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        \n",
    "        batch = int(x.shape[0]/b)\n",
    "        \n",
    "        for i in range(b):\n",
    "            x_batch = x[(batch*i):(batch*(1+i))]\n",
    "            y_batch = y[(batch*i):(batch*(1+i))]\n",
    "            \n",
    "            y_hat = sigmoid_fuction(p.sum(np.transpose(w) * batch_x, axis=1))\n",
    "            \n",
    "            w = w - lr * gradient_lr(y_batch, y_hat, x_batch)\n",
    "    \n",
    "        cost_ep = costf_lr(y, sigmoid_fuction(p.sum(np.transpose(w) * x, axis=1)))\n",
    "        cost.append(cost_ep)\n",
    "        \n",
    "        print(f\"Epoch: {epoch}, Loss: {cost_ep}\")\n",
    "        \n",
    "    return w, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementar función predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar la función predict usando los parámetros calculados y la función sigmoid. Prestar atención a las transformaciones de los datos de entrada. Asimismo, se debe tomar una decisión respecto de los valores de salida como: $p\\geq 0.5 \\to 1, p<0.5 \\to 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23442129  0.11236617  0.43918425 -0.38727535  0.21859714  0.23440551\n",
      " -0.12281028 -0.07421201  0.46689663  0.34488607 -0.11298151  0.41958222\n",
      "  0.46858392  0.29397831 -0.22571511  0.13012009 -0.05463674  0.38732071\n",
      "  0.13218981 -0.41459311  0.35955672 -0.0925815   0.31537811  0.04549787\n",
      " -0.24903804  0.19418298 -0.25087478  0.33530689  0.35987127 -0.16917897\n",
      " -0.05047436 -0.020338   -0.49508386 -0.09897548 -0.26688851 -0.0747939\n",
      " -0.33334849  0.29253698  0.00595834  0.33826269 -0.206502   -0.26537595\n",
      " -0.18497652  0.47193408  0.22473657 -0.11457495  0.42425106  0.39256133\n",
      " -0.28562999  0.17135769]\n",
      "[0.55833841 0.52806202 0.60806464 0.40437338 0.5544327  0.55833452\n",
      " 0.46933596 0.48145551 0.61464897 0.58537692 0.47178463 0.60338328\n",
      " 0.61504853 0.57296981 0.44380958 0.5324842  0.48634421 0.59563755\n",
      " 0.53299941 0.39781129 0.58893312 0.47687114 0.57819745 0.51137251\n",
      " 0.43806028 0.54839378 0.4376082  0.58305006 0.58900927 0.45780585\n",
      " 0.48738409 0.49491567 0.37869667 0.47527631 0.43367112 0.48131024\n",
      " 0.41742611 0.57261711 0.50148958 0.58376845 0.44855718 0.43404264\n",
      " 0.45388728 0.61584142 0.55594886 0.47138756 0.60450004 0.59689913\n",
      " 0.42907405 0.5427349 ]\n",
      "[ True  True  True False  True  True False False  True  True False  True\n",
      "  True  True False  True False  True  True False  True False  True  True\n",
      " False  True False  True  True False False False False False False False\n",
      " False  True  True  True False False False  True  True False  True  True\n",
      " False  True]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "def predict(x, p):\n",
    "    return (sigmoid_fuction(x) > p)\n",
    "\n",
    "x = np.random.rand(50)-0.5\n",
    "print(x)\n",
    "print(sigmoid_fuction(x))\n",
    "print(predict(x, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armar una clase LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armar una clase LogisticRegression que herede de BaseModel y tenga la siguiente estructura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-fd2d64bb5a5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseModel' is not defined"
     ]
    }
   ],
   "source": [
    "class LogisticRegression(BaseModel):\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return NotImplemented\n",
    "\n",
    "    def fit(self, X, y, lr, b, epochs, bias=True):\n",
    "        #self.model = W\n",
    "        return NotImplemented\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testear con Datasets sintéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería Scikit-Learn tiene una función make_classification que nos permite armar datasets de prueba para problemas de clasificación. Prueben con datasets que tengan varios clusters por clase, que tengan menor o mayor separación y calculen las métricas en cada caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "# X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
