{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de éste ejercicio es que implementen paso a paso los building blocks del modelo de regresión logística, para finalmente crear una clase del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos las Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de Building Blocks del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se deberán implementar paso a paso los distintos bloques de código que conforman el modelo, junto con algunas funciones auxiliares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar la función: $g(z) = \\frac{1}{1 + e^{-z}}$ en NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_fuction(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar la función de costo: $J(w) = \\frac{1}{n}\\sum_{i=1}^{n}L\\left ( \\hat{y},y \\right )= \\frac{-1}{n}\\sum_{i=1}^{n}\\left [y^{(i)}log(\\hat{y}^{(i)})+ (1-y^{(i)})log(1-\\hat{y}^{(i)}) \\right ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costf_lr(y, y_hat):\n",
    "    return (-1)*np.mean(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar el gradiente de la función costo respecto de los parámetros: $\\frac{\\partial J(w)}{\\partial w} = \\frac{2}{n}\\sum_{i=1}^{n}\\left ( \\hat{y}^{i}-y^{i}\\right )\\bar{x}^i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_lr(y, y_hat, x):\n",
    "    # y* = (y_hat-y) dimensiones nx1\n",
    "    # x dimensiones nxm\n",
    "    # np.dot(y*.T,x) = y*1 . x1 + ... + y*n . xn \n",
    "    return np.mean((y_hat-y).T.dot(x)) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar normalización Z-score de las features de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    # centro el dataset\n",
    "    x_cent = x.copy()\n",
    "    return (x_cent - np.mean(x, axis=0))/np.std(x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas (Precision, Recall y Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar las métricas en NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_pra(truth, pred):\n",
    "    # Calculo el valor True Positive\n",
    "    true_positive = sum(np.logical_and(truth, pred))\n",
    "\n",
    "    # Calculo el valor True Negative\n",
    "    true_negative = sum(np.logical_and(np.logical_not(truth), np.logical_not(pred)))\n",
    "\n",
    "    # Calculo el valor False Negative\n",
    "    false_negative = sum(np.logical_and(truth, np.logical_not(pred)))\n",
    "\n",
    "    # Calculo el valor False Positive\n",
    "    false_positive = sum(np.logical_and(np.logical_not(truth), pred))\n",
    "\n",
    "    # Metricas\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n",
    "\n",
    "    return true_positive, true_negative, false_negative, false_positive, precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementar función fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizas los bloques anteriores, junto con la implementación en NumPy del algoritmo Mini-Batch gradient descent, para crear la función fit de nuestro modelo de regresión logística. Cada un determinado número de epochs calculen el loss, almacénenlo en una lista y hagan un log de los valores. La función debe devolver los parámetros ajustados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def fit(self, x, y, lr, b, epochs, bias=True):\n",
    "    \n",
    "    cost = []\n",
    "    \n",
    "    if bias:\n",
    "        x = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "    \n",
    "    w = np.random.randn(m).reshape(x.shape[1], 1)\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        \n",
    "        batch = int(x.shape[0]/b)\n",
    "        \n",
    "        for i in range(b):\n",
    "            x_batch = x[(batch*i):(batch*(1+i))]\n",
    "            y_batch = y[(batch*i):(batch*(1+i))]\n",
    "            \n",
    "            y_hat = sigmoid_fuction(p.sum(np.transpose(w) * batch_x, axis=1))\n",
    "            \n",
    "            w = w - lr * gradient_lr(y_batch, y_hat, x_batch)\n",
    "    \n",
    "        cost_ep = costf_lr(y, sigmoid_fuction(p.sum(np.transpose(w) * x, axis=1)))\n",
    "        cost.append(cost_ep)\n",
    "        \n",
    "        print(f\"Epoch: {epoch}, Loss: {cost_ep}\")\n",
    "        \n",
    "    return w, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementar función predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar la función predict usando los parámetros calculados y la función sigmoid. Prestar atención a las transformaciones de los datos de entrada. Asimismo, se debe tomar una decisión respecto de los valores de salida como: $p\\geq 0.5 \\to 1, p<0.5 \\to 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00986509  0.0293902   0.08034042  0.45792508 -0.40291065 -0.38675786\n",
      "  0.43733592  0.31168414  0.40746801  0.05792293 -0.24138668  0.4078045\n",
      " -0.48559839  0.21721412 -0.10059986  0.45017522  0.44744447 -0.41602475\n",
      " -0.10268767  0.09987214 -0.05956453 -0.26149514 -0.43195845 -0.49281105\n",
      " -0.49163995 -0.07644009  0.06558328 -0.4506221   0.37789572 -0.40097879\n",
      " -0.06794435  0.26347694  0.4522364  -0.13666526  0.288447   -0.07184808\n",
      "  0.32209502  0.30292628 -0.3526911  -0.34371165 -0.22517817 -0.4180558\n",
      " -0.21097953  0.02226194 -0.00603562 -0.26299102 -0.07048236 -0.15626708\n",
      " -0.27163554  0.26396552]\n",
      "[0.49753375 0.50734702 0.52007431 0.61252183 0.40061323 0.40449802\n",
      " 0.60762405 0.57729629 0.6004806  0.51447669 0.43994465 0.60056132\n",
      " 0.38093102 0.55409102 0.47487123 0.61068089 0.61003146 0.39746838\n",
      " 0.47435062 0.5249473  0.48511327 0.4349962  0.39365877 0.37923158\n",
      " 0.37950731 0.48089928 0.51638994 0.38921287 0.59336548 0.4010772\n",
      " 0.48302044 0.56549081 0.61117083 0.46588676 0.57161589 0.4820457\n",
      " 0.57983474 0.57515772 0.41272999 0.41490816 0.44394213 0.39698207\n",
      " 0.4474499  0.50556526 0.4984911  0.43462859 0.4823867  0.46101254\n",
      " 0.43250562 0.56561085]\n",
      "[False  True  True  True False False  True  True  True  True False  True\n",
      " False  True False  True  True False False  True False False False False\n",
      " False False  True False  True False False  True  True False  True False\n",
      "  True  True False False False False False  True False False False False\n",
      " False  True]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "def predict(x, p):\n",
    "    return (sigmoid_fuction(x) > p)\n",
    "\n",
    "x = np.random.rand(50)-0.5\n",
    "print(x)\n",
    "print(sigmoid_fuction(x))\n",
    "print(predict(x, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armar una clase LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armar una clase LogisticRegression que herede de BaseModel y tenga la siguiente estructura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(BaseModel):\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return NotImplemented\n",
    "\n",
    "    def fit(self, X, y, lr, b, epochs, bias=True):\n",
    "        #self.model = W\n",
    "        return NotImplemented\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testear con Datasets sintéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería Scikit-Learn tiene una función make_classification que nos permite armar datasets de prueba para problemas de clasificación. Prueben con datasets que tengan varios clusters por clase, que tengan menor o mayor separación y calculen las métricas en cada caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "# X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
